{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(\"___imported___\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# with open('fpmsdb.json', 'r', encoding='utf-8') as f:\n",
    "#     data = json.load(f)\n",
    "    \n",
    "with open('fpmsdoecedb.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "# print(data['features'][4115]['properties']['title'])\n",
    "\n",
    "research_titles = []\n",
    "for i in range(0, 565):\n",
    "#     research_titles.append(data['features'][i]['properties']['title'])\n",
    "    research_titles.append(data[i]['title'])\n",
    "\n",
    "print(research_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing & NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download necessary resources from NLTK\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# define stop words and lemmatizer\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# preprocess function for text data\n",
    "def preprocess(text):\n",
    "    # tokenize the text and remove stop words and punctuation\n",
    "    tokens = [word for word in nltk.word_tokenize(text.lower()) if word not in stop and word not in exclude]\n",
    "    \n",
    "    # extract collocations and add them to the token list\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(tokens)\n",
    "    finder.apply_freq_filter(2)  # only consider bigrams that occur at least twice\n",
    "    collocations = finder.nbest(bigram_measures.pmi, 10)  # extract the top 10 collocations\n",
    "    for collocation in collocations:\n",
    "        if collocation[0] in tokens and collocation[1] in tokens:\n",
    "            tokens.append('_'.join(collocation))\n",
    "    \n",
    "    # lemmatize the tokens\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in tokens)\n",
    "    return normalized\n",
    "\n",
    "# preprocess the titles\n",
    "titles_preprocessed = [preprocess(title) for title in research_titles]\n",
    "\n",
    "# create a dictionary and corpus\n",
    "dictionary = corpora.Dictionary([doc.split() for doc in titles_preprocessed])\n",
    "corpus = [dictionary.doc2bow(doc.split()) for doc in titles_preprocessed]\n",
    "\n",
    "# build the LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=dictionary,\n",
    "                                            num_topics=20,\n",
    "                                            random_state=100,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=100,\n",
    "                                            passes=10,\n",
    "                                            alpha='auto', # higher value => more concentrated topics\n",
    "                                            per_word_topics=True)\n",
    "\n",
    "# print the top 10 topics and their most significant words\n",
    "topics = lda_model.show_topics(num_topics=20, num_words=3, formatted=False)\n",
    "for topic in topics:\n",
    "    print(\"Topic {}: {}\".format(topic[0], \", \".join([word[0] for word in topic[1]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Highest Probability Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorize the research titles based on their highest probability topic\n",
    "for i, title in enumerate(titles_preprocessed):\n",
    "    bow = dictionary.doc2bow(preprocess(title).split())\n",
    "    topic_probs = lda_model.get_document_topics(bow)\n",
    "    topic_probs_sorted = sorted(topic_probs, key=lambda x: x[1], reverse=True)\n",
    "    topic_num = topic_probs_sorted[0][0]\n",
    "    print(research_titles[i])\n",
    "    print(\"{} - Topic {}: {}\".format(i+1, topic_num, \", \".join([word[0] for word in topics[topic_num][1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'tuple' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# categorize the titles into clusters using the LDA model\u001b[39;00m\n\u001b[0;32m     13\u001b[0m corpus_lda \u001b[38;5;241m=\u001b[39m lda_model[corpus]\n\u001b[1;32m---> 14\u001b[0m doc_lda \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mmax\u001b[39m(prob, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m y: y[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m prob \u001b[38;5;129;01min\u001b[39;00m corpus_lda]\n\u001b[0;32m     15\u001b[0m doc_lda \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(doc_lda)\n\u001b[0;32m     16\u001b[0m clusters \u001b[38;5;241m=\u001b[39m DBSCAN(eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mfit_predict(doc_lda[:, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "Cell \u001b[1;32mIn[62], line 14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# categorize the titles into clusters using the LDA model\u001b[39;00m\n\u001b[0;32m     13\u001b[0m corpus_lda \u001b[38;5;241m=\u001b[39m lda_model[corpus]\n\u001b[1;32m---> 14\u001b[0m doc_lda \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m prob \u001b[38;5;129;01min\u001b[39;00m corpus_lda]\n\u001b[0;32m     15\u001b[0m doc_lda \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(doc_lda)\n\u001b[0;32m     16\u001b[0m clusters \u001b[38;5;241m=\u001b[39m DBSCAN(eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mfit_predict(doc_lda[:, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'tuple' and 'int'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# preprocess the titles\n",
    "titles_preprocessed = [preprocess(title) for title in research_titles]\n",
    "\n",
    "# create a document-term matrix using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(titles_preprocessed)\n",
    "\n",
    "# categorize the titles into clusters using the LDA model\n",
    "corpus_lda = lda_model[corpus]\n",
    "doc_lda = [max(prob, key=lambda y: y[1]) for prob in corpus_lda]\n",
    "doc_lda = np.array(doc_lda)\n",
    "clusters = DBSCAN(eps=0.7, min_samples=2).fit_predict(doc_lda[:, 1].reshape(-1, 1))\n",
    "\n",
    "# print the research titles with their cluster name\n",
    "for i, title in enumerate(research_titles):\n",
    "    print(\"Title: {}\\nCluster: {}\\n\".format(title, clusters[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract topic probabilities for each document\n",
    "doc_topic_probs = []\n",
    "for doc in corpus:\n",
    "    topic_probs = lda_model.get_document_topics(doc)\n",
    "    topic_probs_dict = {topic_num: prob for topic_num, prob in topic_probs}\n",
    "    doc_topic_probs.append(topic_probs_dict)\n",
    "\n",
    "# convert the topic probabilities to a numpy array\n",
    "doc_topic_probs_array = np.array(doc_topic_probs)\n",
    "\n",
    "# use DBSCAN to cluster the documents based on their topic probabilities\n",
    "dbscan = DBSCAN(eps=0.15, min_samples=5)\n",
    "dbscan.fit(doc_topic_probs_array)\n",
    "\n",
    "# print the clusters\n",
    "for i, label in enumerate(dbscan.labels_):\n",
    "    print(\"{} - Cluster {}\".format(i+1, label))\n",
    "    print(research_titles[i])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
