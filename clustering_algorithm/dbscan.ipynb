{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(\"___imported___\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# with open('fpmsdb.json', 'r', encoding='utf-8') as f:\n",
    "#     data = json.load(f)\n",
    "    \n",
    "with open('fpmsdoecedb.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "# print(data['features'][4115]['properties']['title'])\n",
    "\n",
    "research_titles = []\n",
    "for i in range(0, 565):\n",
    "#     research_titles.append(data['features'][i]['properties']['title'])\n",
    "    research_titles.append(data[i]['title'])\n",
    "\n",
    "print(research_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing & NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download necessary resources from NLTK\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# define stop words and lemmatizer\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# preprocess function for text data\n",
    "def preprocess(text):\n",
    "    # tokenize the text and remove stop words and punctuation\n",
    "    tokens = [word for word in nltk.word_tokenize(text.lower()) if word not in stop and word not in exclude]\n",
    "    \n",
    "    # extract collocations and add them to the token list\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(tokens)\n",
    "    finder.apply_freq_filter(2)  # only consider bigrams that occur at least twice\n",
    "    collocations = finder.nbest(bigram_measures.pmi, 10)  # extract the top 10 collocations\n",
    "    for collocation in collocations:\n",
    "        if collocation[0] in tokens and collocation[1] in tokens:\n",
    "            tokens.append('_'.join(collocation))\n",
    "    \n",
    "    # lemmatize the tokens\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in tokens)\n",
    "    return normalized\n",
    "\n",
    "# preprocess the titles\n",
    "titles_preprocessed = [preprocess(title) for title in research_titles]\n",
    "\n",
    "# create a dictionary and corpus\n",
    "dictionary = corpora.Dictionary([doc.split() for doc in titles_preprocessed])\n",
    "corpus = [dictionary.doc2bow(doc.split()) for doc in titles_preprocessed]\n",
    "\n",
    "# build the LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=dictionary,\n",
    "                                            num_topics=20,\n",
    "                                            random_state=100,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=100,\n",
    "                                            passes=10,\n",
    "                                            alpha='auto', # higher value => more concentrated topics\n",
    "                                            per_word_topics=True)\n",
    "\n",
    "# print the top 10 topics and their most significant words\n",
    "topics = lda_model.show_topics(num_topics=20, num_words=3, formatted=False)\n",
    "for topic in topics:\n",
    "    print(\"Topic {}: {}\".format(topic[0], \", \".join([word[0] for word in topic[1]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Highest Probability Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorize the research titles based on their highest probability topic\n",
    "for i, title in enumerate(titles_preprocessed):\n",
    "    bow = dictionary.doc2bow(preprocess(title).split())\n",
    "    topic_probs = lda_model.get_document_topics(bow)\n",
    "    topic_probs_sorted = sorted(topic_probs, key=lambda x: x[1], reverse=True)\n",
    "    topic_num = topic_probs_sorted[0][0]\n",
    "    print(research_titles[i])\n",
    "    print(\"{} - Topic {}: {}\".format(i+1, topic_num, \", \".join([word[0] for word in topics[topic_num][1]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract topic probabilities for each document\n",
    "doc_topic_probs = []\n",
    "for doc in corpus:\n",
    "    topic_probs = lda_model.get_document_topics(doc)\n",
    "    topic_probs_dict = {topic_num: prob for topic_num, prob in topic_probs}\n",
    "    doc_topic_probs.append(topic_probs_dict)\n",
    "\n",
    "# convert the topic probabilities to a numpy array\n",
    "doc_topic_probs_array = np.array(doc_topic_probs)\n",
    "\n",
    "# use DBSCAN to cluster the documents based on their topic probabilities\n",
    "dbscan = DBSCAN(eps=0.15, min_samples=5)\n",
    "dbscan.fit(doc_topic_probs_array)\n",
    "\n",
    "# print the clusters\n",
    "for i, label in enumerate(dbscan.labels_):\n",
    "    print(\"{} - Cluster {}\".format(i+1, label))\n",
    "    print(research_titles[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m topic_probabilities \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(research_titles)):\n\u001b[1;32m----> 4\u001b[0m     topic_probabilities\u001b[38;5;241m.\u001b[39mappend([t[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m lda_model[corpus[i]]])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# cluster the titles into k clusters based on the topic probabilities\u001b[39;00m\n\u001b[0;32m      7\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m  \u001b[38;5;66;03m# same as the number of LDA topics\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[67], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m topic_probabilities \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(research_titles)):\n\u001b[1;32m----> 4\u001b[0m     topic_probabilities\u001b[38;5;241m.\u001b[39mappend([\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m lda_model[corpus[i]]])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# cluster the titles into k clusters based on the topic probabilities\u001b[39;00m\n\u001b[0;32m      7\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m  \u001b[38;5;66;03m# same as the number of LDA topics\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# convert the LDA model output to a matrix of topic probabilities for each title\n",
    "topic_probabilities = []\n",
    "for i in range(len(research_titles)):\n",
    "    topic_probabilities.append([t[1] for t in lda_model[corpus[i]]])\n",
    "\n",
    "# cluster the titles into k clusters based on the topic probabilities\n",
    "k = 20  # same as the number of LDA topics\n",
    "kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "clusters = kmeans.fit_predict(topic_probabilities)\n",
    "\n",
    "# print the titles and their assigned clusters\n",
    "for i in range(len(research_titles)):\n",
    "    print(\"{}\\tCluster {}\".format(research_titles[i], clusters[i]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
