{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('fpmsdb.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "# print(data['features'][4115]['properties']['title'])\n",
    "\n",
    "research_titles = []\n",
    "for i in range(0, 4115):\n",
    "    research_titles.append(data['features'][i]['properties']['title'])\n",
    "\n",
    "print(research_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: detection, cnn, classification\n",
      "Topic 1: district, improvement, behavior\n",
      "Topic 2: learning, deep, algorithm\n",
      "Topic 3: model, web, prediction\n",
      "Topic 4: assessment, satellite, smart\n",
      "Topic 5: local, status, glacier\n",
      "Topic 6: nepal, kathmandu, valley\n",
      "Topic 7: system, communication, time\n",
      "Topic 8: new, data, road\n",
      "Topic 9: recognition, plant, disease\n",
      "Topic 10: building, design, climate\n",
      "Topic 11: analysis, case, potential\n",
      "Topic 12: performance, development, thermal\n",
      "Topic 13: review, application, different\n",
      "Topic 14: using, estimation, vehicle\n",
      "Topic 15: network, impact, neural\n",
      "Topic 16: energy, management, security\n",
      "Topic 17: urban, towards, â€™\n",
      "Topic 18: study, challenge, perspective\n",
      "Topic 19: based, approach, related\n"
     ]
    }
   ],
   "source": [
    "# download necessary resources from NLTK\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# define stop words and lemmatizer\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# preprocess function for text data\n",
    "def preprocess(text):\n",
    "    # tokenize the text and remove stop words and punctuation\n",
    "    tokens = [word for word in nltk.word_tokenize(text.lower()) if word not in stop and word not in exclude]\n",
    "    \n",
    "    # extract collocations and add them to the token list\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(tokens)\n",
    "    finder.apply_freq_filter(2)  # only consider bigrams that occur at least twice\n",
    "    collocations = finder.nbest(bigram_measures.pmi, 10)  # extract the top 10 collocations\n",
    "    for collocation in collocations:\n",
    "        if collocation[0] in tokens and collocation[1] in tokens:\n",
    "            tokens.append('_'.join(collocation))\n",
    "    \n",
    "    # lemmatize the tokens\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in tokens)\n",
    "    return normalized\n",
    "\n",
    "# preprocess the titles\n",
    "titles_preprocessed = [preprocess(title) for title in research_titles]\n",
    "\n",
    "# create a dictionary and corpus\n",
    "dictionary = corpora.Dictionary([doc.split() for doc in titles_preprocessed])\n",
    "corpus = [dictionary.doc2bow(doc.split()) for doc in titles_preprocessed]\n",
    "\n",
    "# build the LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=dictionary,\n",
    "                                            num_topics=20,\n",
    "                                            random_state=100,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=100,\n",
    "                                            passes=10,\n",
    "                                            alpha='auto', # higher value => more concentrated topics\n",
    "                                            per_word_topics=True)\n",
    "\n",
    "# print the top 10 topics and their most significant words\n",
    "topics = lda_model.show_topics(num_topics=20, num_words=3, formatted=False)\n",
    "for topic in topics:\n",
    "    print(\"Topic {}: {}\".format(topic[0], \", \".join([word[0] for word in topic[1]])))\n",
    "    \n",
    "# categorize the research titles based on their highest probability topic\n",
    "# for i, title in enumerate(titles_preprocessed):\n",
    "#     bow = dictionary.doc2bow(preprocess(title).split())\n",
    "#     topic_probs = lda_model.get_document_topics(bow)\n",
    "#     topic_probs_sorted = sorted(topic_probs, key=lambda x: x[1], reverse=True)\n",
    "#     topic_num = topic_probs_sorted[0][0]\n",
    "#     print(\"{} - Topic {}: {}\".format(i+1, topic_num, \", \".join([word[0] for word in topics[topic_num][1]])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
